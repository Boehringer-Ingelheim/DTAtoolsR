---
title: "DTAtools"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{DTAtools}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options:
  markdown:
    wrap: 80
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Introduction

`DTAtools` is an R package designed to streamline the validation and management of **Data Transfer Agreements (DTAs)** using YAML-based specifications and schema rules. By automating the validation process, `DTAtools` ensures data integrity, reduces manual errors, and accelerates the secure exchange of data between stakeholders.

### Key Features

- **YAML-based specifications:** Define column-level constraints, schema rules, and metadata in a human-readable format.
- **Validation engine:** Validate tabular data against column specifications and schema rules.
- **Export tools:** Generate Word documents for DTA specifications and validation results.
- **Error handling:** Detect and report validation errors for quick troubleshooting.

### Background

**Data Transfer Agreements (DTAs)** or **Data Transfer Specifications (DTS)** are critical for ensuring the secure and compliant exchange of data between stakeholders, such as research organizations, vendors, and pharmaceutical companies. DTAs play a pivotal role in clinical and regulatory environments, where data integrity and adherence to specifications are paramount.

However, the manual process of validating these data transfers is often error-prone and time-consuming. Common challenges include:

- Repeated submissions due to validation errors.
- Misalignment between data formats and agreed specifications.
- Inefficient workflows for detecting and resolving issues.

To address these challenges, `DTAtools` provides a comprehensive solution that automates the validation process, enforces schema-based rules, and generates human-readable documentation. By leveraging YAML for specifications and integrating validation workflows, `DTAtools` empowers users to ensure data compliance efficiently and effectively.

## Features

- Import/export DTA specifications from/to YAML and Word documents
- Comprehensive validation of data for: type, format, nullability, allowed
  values, and regex patterns
- Validation of schema rules (column based logic)

## Installation

To install the `DTAtools` package, you can currently only install the development version from GitHub. Follow the steps below:

Before installing `DTAtools`, ensure you have the following packages installed:

- **`remotes`**: Used for installing packages from GitHub.

1. Install the `remotes` package if it is not already installed.
2. Install `DTAtools`
3. Load `DTAtools`

```{r, eval=FALSE}
install.packages("remotes")
remotes::install_github("Boehringer-Ingelheim/dtatools")
library(DTAtools)
```

#### Troubleshooting

If you encounter any issues during installation:

Missing system libraries: For example, jsonvalidate may require libcurl. Install the necessary libraries for your operating system. On Ubuntu/Debian: sudo apt-get install libcurl4-openssl-dev On macOS: Use brew install curl R version compatibility: Ensure your R version is 3.5.0 or higher.

## Quickstart

The `DTAtools` package provides tools to validate data against specifications defined in YAML format. This quickstart guide walks you through loading data, validating it, handling errors, and interpreting the results.

---

### Step 1: Load Required Libraries

First, load the `DTAtools` package and any other required libraries:

```{r eval=FALSE}
# Load DTAtools
library(DTAtools)

# Additional libraries (if needed)
library(data.table) # For reading tabular data
```

### Step 2: Load Specifications and Data

Prepare your data and specifications:

- **Specifications**: A YAML file (`specs.yaml`) defining the column-level constraints and schema rules.
- **Data**: A tabular file (`data.tsv`) containing the data to validate.

Load the specifications and data:

```{r eval=FALSE}
# Load specifications from a YAML file
specs <- importDTAColumnSpecCollectionFromYaml("specs.yaml")

# Load data from a TSV file
data <- data.table::fread("data.tsv")
```

### Step 3: Validate the Data

Use the `DTAContainer` function to validate the data against the specifications:

```{r eval=FALSE}
# Validate the data
dt <- DTAContainer(specs, list(my_data = data))
```

### Step 4: Handle Validation Results

#### Success

A `table is valid` message is printed and the tables structure and values conforms to the specification.

## Usage

### Import DTA specifications

DTA / DTS specifications are stored in the human and machine readable YAML format. Generally, they are store in either dedicated YAML files for are part of YAML configuration files.

Specification contain definitions of:

- **columns:** defines column names, format specification and optionally patterns or selection for values in the column. More details in section [YAML Column Format](#yaml-column-format).
- **rules:** defines rules for columns, e.g. if column A is empty, then columns B must contain a value. More details in section [YAML Schema Rule Specification](#yaml-schema-rule-specification).
- **metadata:** DTA meta data about version, author etc. More details in section [YAML Metadata]({#yaml-metadata}).

First, you import the specifications from a YAML file.

```{r}
library(DTAtools)
params_yaml <- system.file("extdata", "params_gf.yaml", package = "DTAtools")

specs <- importDTAColumnSpecCollectionFromYaml(params_yaml)
```

> Note: Use the `tag` argument of `importDTAColumnSpecCollectionFromYaml` if `columns`, `rules` and `metadata` are nested in the YAML file, e.g. if they are under `DTA: format:`, then add `tag=c("DTA", "format")`.

### Load and validate data

The next step is to import the data and create a `DTAContainer` object. Once created, the `DTAContainer` object will validate the data for the specifications right away.

Here we load example data. We recommend to load large tables with `fread` or
small to medium sized data with `tibble` related functions like `read_csv` or
`read_tsv` from `tidyverse`.

```{r load example data}
example_file <- system.file("extdata", "data_gf_small.tsv", package="DTAtools")

example_data <- read.table(example_file, sep = "\t", header = T)
```

The data along with DTA column specifications is used to create a DTAContainer
object. Upon creation, the data is checked for validity using the extremely
efficient `jsonschema` package.

```{r}
data <- DTAContainer(
  specs = specs,
  data = list("example_data" = example_data)
)
```

### Check semantics with schema rules

TODO!

```{r, eval=FALSE}
#applySchemaRules()
#validateTable(column_collection, table)

#exportDTASpecTable(column_collection, overwrite = TRUE)

#imported_collection <- importDTAColumnSpecCollectionFromDTA("imported_collection.docx", colnames = c("id", "label", "type", "format", "nullable", "description"))

# exportDTASpecTable(imported_collection, file = "imported_collection.docx")
# exportDTASpecTable(imported_collection, file = "imported_collection_new.docx", overwrite = TRUE)

# dtatools <- DTAContainer(DTAColumnSpecCollection = imported_collection, tables = list("DTA" = table))

writeTableToFile(
  dtatools,
  "DTA",
  filename = "test.csv",
  compression = "none",
  row.names = FALSE, quote = FALSE,
  overwrite = TRUE
)
```

#### Write specs

TODO!

```{r, eval=FALSE}

writeDTAColumnSpecCollectionToYaml(column_collection, "test_params.yaml")

column_collection <- importDTAColumnSpecCollectionFromYaml("test_params.yaml")

writeTableToFile(dtatools, "DTA")
```

## Additional features

### Export Spec Table to Word

The `exportDTASpecTable()` function generates a Word document containing a table of all specifications defined in the YAML file. The table can be directly imported into a DTA Word document.

- **Inputs**:
  - `specs`: The specifications object loaded from a YAML file, which defines the column constraints and schema rules.
- **Output**:
  - A Word document (`dta_spec_table.docx`) containing a table summarizing all specifications.

#### Example Usage:

The following code exports the specifications to a Word document:

```{r, eval=FALSE}
exportDTASpecTable(specs, "dta_spec_table.docx")
```

### Export Column Values Table

The `exportColumnValueTable()` function generates a Word document containing a table of all possible values for specified columns based on the provided specifications. This is particularly useful for reviewing or sharing the expected values for specific columns in your dataset.

- **Inputs**:
  - `specs`: The specifications object loaded from a YAML file, which defines the column constraints and expected values.
  - `id`: The column identifier for which the values should be exported (e.g., `"VISIT"`).
- **Output**:
  - A Word document (`column_value_table.docx`) containing a table of possible values for the specified column.

```{r, eval=FALSE}
exportColumnValueTable(specs, "column_value_table.docx", id = "VISIT")
```

## YAML Column Format {#yaml-column-format}

Columns specifications can contain

- _id:_ (mandatory) the ID of the column
- _label:_ (mandatory) the label of columns
- _description:_ (optional) the description of the column
- _type:_ (mandatory) the SAS type like Num, Char, Date9
- _format:_ (mandatory) the SAS format like 10., \$10
- _nullable:_ (mandatory) if a columns has to contain values (Yes, No or True, False)
- _pattern:_ (optional): Regex for value check
- _values:_ (optional): A list of possible values

Here is an example of a column `SUBJIDN` with a pattern

```{yaml}
columns:
  - id: SUBJIDN
    label: Subject identifier for the study
    type: Num
    format: 10.
    nullable: No
    pattern: "^[0-9]{10}$"
```

Here is an example of a column `GFGRPID` with

```{yaml}
columns:
  - id: SEX
    label: Sex
    type: Char
    format: $6
    nullable: Yes
    description: "Self reported sex, empty if not reported"
    values:
      - "male"
      - "female"
      - "other"
```

## YAML Schema Rule Specification {#yaml-schema-rule-specification}

The `DTAtools` package supports schema-based validation of tabular data using declarative rules defined in YAML. These rules are evaluated after column-level validation and allow for complex inter-column logic enforcement.

### Structure

Rules must be defined under the top-level key `rules` in the YAML file. Each rule is a list item with a required `id`, `type`, and additional fields depending on the rule type.

### Supported Rule Types

#### 1. `check_equal`

Ensures that if a condition is met in one column, another column must equal a specific value.

```{yaml}
rules:
  - id: rule_equal_example
    type: check_equal
    condition:
      column: VISIT
      equals: "V03"
    then:
      column: STATUS
      equals: "COMPLETED"
```

#### 2. `check_equal`

Ensures that if a condition is met in one column, another column must not equal a specific value.

```{yaml}
rules:
  - id: rule_unequal_example
    type: check_unequal
    condition:
      column: VISIT
      equals: "V03"
    then:
      column: STATUS
      equals: "DROPPED"
```

#### 3. `check_range`

Ensures that values in a numeric column fall within a specified range.

```{yaml}
rules:
  - id: rule_range_example
    type: check_range
    column: AGE
    range: [18, 65]
```

#### 4. `check_dependency`

Ensures that if a condition is met, another column must not be null, empty, or NaN.

```{yaml}
rules:
  - id: rule_dependency_example
    type: check_dependency
    condition:
      column: CONSENT
      equals: "YES"
    then:
      column: CONSENT_DATE
```

#### 5. `check_mutual_exclusive`

Ensures that two columns are not both populated in the same row.

```{yaml}
rules:
  - id: rule_mutual_exclusive_example
    type: check_mutual_exclusive
    columns: [AE_TERM, SAE_TERM]
```

#### 6. `check_unique`

Ensures that all values in a column are unique.

```{yaml}
rules:
  - id: rule_unique_example
    type: check_unique
    column: SUBJECT_ID
```

#### 7. `check_allowed_combinations`

Ensures that only specific combinations of values across multiple columns are allowed.

```{yaml}
rules:
  - id: rule_allowed_combinations_example
    type: check_allowed_combinations
    columns: [VISIT, STATUS]
    allowed:
      - ["V01", "COMPLETED"]
      - ["V02", "DROPPED"]
```

## YAML Metadata {#yaml-metadata}

The **YAML Metadata** section provides important information about the dataset, including versioning, authorship, and other descriptive details. This metadata is stored in the YAML file alongside column specifications and serves as a reference for understanding the dataset’s context and provenance. Potentially, the metadata could be used to create a full DTA in combination with the `YAML column specifications`.

### Structure of YAML Metadata

The metadata section in the YAML file typically includes the following fields:

Metadata can contain

- **version:** Version of specifications
- **author:** Author
- **create:** Date of creation
- **description:** Description of specifications

Here’s an example of how the metadata section might look in a YAML file:

```{yaml}
metadata:
- version: "1.0.0"
  author: "Thomas Schwarzl"
  created: "2025-07-08"
  description: "GF Domain Specification"
```

#### Accessing Metadata in R

The `getMetadata()` method returns a list containing the metadata fields. For example:

```{r}
# Access metadata
metadata <- DTAtools:::getMetadata(specs)

# Print metadata
print(metadata)
```

## Technical Overview

`DTAtools` is built on the S7 object system and leverages `jsonschema` for data validation. Additionally it integrates logic to make cross-column validity checks and extend the features of `jsonschema`. Generally, it provides a structured approach to defining, validating, and exporting specifications for tabular datasets.

### Core Concepts

- **Column Specifications**: Define individual column metadata and constraints using `DTAColumnSpec`.
- **Specification Collections**: Group multiple column specifications into collections using `DTAColumnSpecCollection`.
- **Data Validation**: Validate data frames against specifications using `DTAContainer`.
- **Documentation Export**: Generate documentation tables in Word format using `flextable`.

### Core Classes

- **`DTAColumnSpec`**: Represents the metadata and constraints for a single column.
- **`DTAColumnSpecCollection`**: A named collection of `DTAColumnSpec` objects.
- **`DTAContainer`**: A container for validating data frames against a specification collection.

### Validation Functions

`DTAtools` provides a range of functions for validating data:

- `validateTable()`: Validates an entire data frame against a specification collection using `jsonschema`.
- `applySchemaRules` validates the data by applying the defined schema rules 

#### Rules Engine

The rules engine supports a variety of validation types to ensure data integrity:

- **Implemented Rule Types**:
  - `check_equal`: Ensures values match a specific target.
  - `check_unequal`: Ensures values do not match a specific target.
  - `check_range`: Validates values within a defined range.
  - `check_dependency`: Ensures values meet dependency conditions.
  - `check_mutual_exclusive`: Ensures mutually exclusive values across columns.
  - `check_unique`: Validates uniqueness of values within a column.
  - `check_allowed_combinations`: Ensures values match allowed combinations across columns.

### Export Functions

`DTAtools` includes functions to export validated data and specifications:

- **Export Validated Data**:

  - `writeTableToFile()`: Writes validated tables to disk with optional compression and metadata.

- **Export Documentation**:
  - `exportDTASpecTable()`: Exports full specification documentation to a Word file.
  - `exportColumnValueTable()`: Exports allowed values for a specific column to a Word file.

### Manually Adding Specifications

You can manually define column specifications using `DTAColumnSpec` and group them into a collection with `DTAColumnSpecCollection`. Here’s an example:

```{r}
# Define individual column specifications

col1 <- DTAColumnSpec(id = "STUDYID", type = "Char", nullable = FALSE)
col2 <- DTAColumnSpec(id = "VISIT", type = "Char", nullable = TRUE)

# Group specifications into a collection

specs <- DTAColumnSpecCollection(columns = list(STUDYID = col1, VISIT = col2))
```

### Important Notes

- **Unique IDs**: All rules must include a unique identifier (`id`).
- **Supported Rule Types**: Rule types must match exactly one of the supported types listed above.
- **Validation Errors**: Missing or malformed rules will trigger validation errors before data evaluation.
- **YAML Import**: When importing YAML from DSO, regex patterns must be non-quoted strings.

## Credits

`DTAtools` was developed by

- Daniel Schreyer
- Thomas Schwarzl
